{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "coursera": {
      "schema_names": [
        "MLEPC2W2-1",
        "MLEPC2W2-2",
        "MLEPC2W2-3",
        "MLEPC2W2-4",
        "MLEPC2W2-5",
        "MLEPC2W2-6",
        "MLEPC2W2-7"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "colab": {
      "name": "C2W2_Assignment.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/poojasaxena/MLOps-specialization/blob/main/course_2_machine_learning_data_lifecycle_in_production/C2W2_Assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "23R0Z9RojXYW"
      },
      "source": [
        "# Week 2 Assignment: Feature Engineering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sfSQ-kX-MLEr"
      },
      "source": [
        "For this week's assignment, you will build a data pipeline using using [Tensorflow Extended (TFX)](https://www.tensorflow.org/tfx) to prepare features from the [Metro Interstate Traffic Volume dataset](https://archive.ics.uci.edu/ml/datasets/Metro+Interstate+Traffic+Volume). Try to only use the documentation and code hints to accomplish the tasks but feel free to review the 2nd ungraded lab this week in case you get stuck.\n",
        "\n",
        "Upon completion, you will have:\n",
        "\n",
        "* created an InteractiveContext to run TFX components interactively\n",
        "* used TFX ExampleGen component to split your dataset into training and evaluation datasets\n",
        "* generated the statistics and the schema of your dataset using TFX StatisticsGen and SchemaGen components\n",
        "* validated the evaluation dataset statistics using TFX ExampleValidator\n",
        "* performed feature engineering using the TFX Transform component\n",
        "\n",
        "Let's begin!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YpBcSBFXrIXd"
      },
      "source": [
        "## Table of Contents\n",
        "\n",
        "- [1 - Setup](#1)\n",
        "  - [1.1 - Imports](#1-1)\n",
        "  - [1.2 - Define Paths](#1-2)\n",
        "  - [1.3 - Preview the Dataset](#1-3)\n",
        "  - [1.4 - Create the InteractiveContext](#1-4)\n",
        "- [2 - Run TFX components interactively](#2)\n",
        "  - [2.1 - ExampleGen](#2-1)\n",
        "    - [Exercise 1 - ExampleGen](#ex-1)\n",
        "    - [Exercise 2 - get_records()](#ex-2)\n",
        "  - [2.2 - StatisticsGen](#2-2)\n",
        "    - [Exercise 3 - StatisticsGen](#ex-3)\n",
        "  - [2.3 - SchemaGen](#2-3)\n",
        "    - [Exercise 4 - SchemaGen](#ex-4)\n",
        "  - [2.4 - ExampleValidator](#2-4)\n",
        "    - [Exercise 5 - ExampleValidator](#ex-5)\n",
        "  - [2.5 - Transform](#2-5)\n",
        "    - [Exercise 6 - preprocessing_fn()](#ex-6)\n",
        "    - [Exercise 7 - Transform](#ex-7)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2GivNBNYjb3b"
      },
      "source": [
        "<a name='1'></a>\n",
        "## 1 - Setup\n",
        "As usual, you will first need to import the necessary packages. For reference, the lab environment uses *TensorFlow version: 2.3.1* and *TFX version: 0.24.0*."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N-ePgV0Lj68Q"
      },
      "source": [
        "<a name='1-1'></a>\n",
        "### 1.1 Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lydvc0wdrQ7X",
        "outputId": "fa86b620-6970-405e-8c1c-6475fcf7a2fb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!pip install tfx"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tfx\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/62/5a/cf16a9aa1f248b7e19673bd7f37e03a5ff8dbaefd54357120d268b9723ca/tfx-0.30.1-py3-none-any.whl (2.4MB)\n",
            "\r\u001b[K     |▏                               | 10kB 13.4MB/s eta 0:00:01\r\u001b[K     |▎                               | 20kB 18.1MB/s eta 0:00:01\r\u001b[K     |▍                               | 30kB 22.9MB/s eta 0:00:01\r\u001b[K     |▌                               | 40kB 26.4MB/s eta 0:00:01\r\u001b[K     |▊                               | 51kB 29.3MB/s eta 0:00:01\r\u001b[K     |▉                               | 61kB 31.2MB/s eta 0:00:01\r\u001b[K     |█                               | 71kB 32.8MB/s eta 0:00:01\r\u001b[K     |█                               | 81kB 29.6MB/s eta 0:00:01\r\u001b[K     |█▏                              | 92kB 30.1MB/s eta 0:00:01\r\u001b[K     |█▍                              | 102kB 31.6MB/s eta 0:00:01\r\u001b[K     |█▌                              | 112kB 31.6MB/s eta 0:00:01\r\u001b[K     |█▋                              | 122kB 31.6MB/s eta 0:00:01\r\u001b[K     |█▊                              | 133kB 31.6MB/s eta 0:00:01\r\u001b[K     |██                              | 143kB 31.6MB/s eta 0:00:01\r\u001b[K     |██                              | 153kB 31.6MB/s eta 0:00:01\r\u001b[K     |██▏                             | 163kB 31.6MB/s eta 0:00:01\r\u001b[K     |██▎                             | 174kB 31.6MB/s eta 0:00:01\r\u001b[K     |██▍                             | 184kB 31.6MB/s eta 0:00:01\r\u001b[K     |██▋                             | 194kB 31.6MB/s eta 0:00:01\r\u001b[K     |██▊                             | 204kB 31.6MB/s eta 0:00:01\r\u001b[K     |██▉                             | 215kB 31.6MB/s eta 0:00:01\r\u001b[K     |███                             | 225kB 31.6MB/s eta 0:00:01\r\u001b[K     |███▏                            | 235kB 31.6MB/s eta 0:00:01\r\u001b[K     |███▎                            | 245kB 31.6MB/s eta 0:00:01\r\u001b[K     |███▍                            | 256kB 31.6MB/s eta 0:00:01\r\u001b[K     |███▌                            | 266kB 31.6MB/s eta 0:00:01\r\u001b[K     |███▋                            | 276kB 31.6MB/s eta 0:00:01\r\u001b[K     |███▉                            | 286kB 31.6MB/s eta 0:00:01\r\u001b[K     |████                            | 296kB 31.6MB/s eta 0:00:01\r\u001b[K     |████                            | 307kB 31.6MB/s eta 0:00:01\r\u001b[K     |████▏                           | 317kB 31.6MB/s eta 0:00:01\r\u001b[K     |████▍                           | 327kB 31.6MB/s eta 0:00:01\r\u001b[K     |████▌                           | 337kB 31.6MB/s eta 0:00:01\r\u001b[K     |████▋                           | 348kB 31.6MB/s eta 0:00:01\r\u001b[K     |████▊                           | 358kB 31.6MB/s eta 0:00:01\r\u001b[K     |████▉                           | 368kB 31.6MB/s eta 0:00:01\r\u001b[K     |█████                           | 378kB 31.6MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 389kB 31.6MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 399kB 31.6MB/s eta 0:00:01\r\u001b[K     |█████▍                          | 409kB 31.6MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 419kB 31.6MB/s eta 0:00:01\r\u001b[K     |█████▊                          | 430kB 31.6MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 440kB 31.6MB/s eta 0:00:01\r\u001b[K     |██████                          | 450kB 31.6MB/s eta 0:00:01\r\u001b[K     |██████                          | 460kB 31.6MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 471kB 31.6MB/s eta 0:00:01\r\u001b[K     |██████▍                         | 481kB 31.6MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 491kB 31.6MB/s eta 0:00:01\r\u001b[K     |██████▋                         | 501kB 31.6MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 512kB 31.6MB/s eta 0:00:01\r\u001b[K     |███████                         | 522kB 31.6MB/s eta 0:00:01\r\u001b[K     |███████                         | 532kB 31.6MB/s eta 0:00:01\r\u001b[K     |███████▏                        | 542kB 31.6MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 552kB 31.6MB/s eta 0:00:01\r\u001b[K     |███████▌                        | 563kB 31.6MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 573kB 31.6MB/s eta 0:00:01\r\u001b[K     |███████▊                        | 583kB 31.6MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 593kB 31.6MB/s eta 0:00:01\r\u001b[K     |████████                        | 604kB 31.6MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 614kB 31.6MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 624kB 31.6MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 634kB 31.6MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 645kB 31.6MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 655kB 31.6MB/s eta 0:00:01\r\u001b[K     |████████▉                       | 665kB 31.6MB/s eta 0:00:01\r\u001b[K     |█████████                       | 675kB 31.6MB/s eta 0:00:01\r\u001b[K     |█████████                       | 686kB 31.6MB/s eta 0:00:01\r\u001b[K     |█████████▏                      | 696kB 31.6MB/s eta 0:00:01\r\u001b[K     |█████████▍                      | 706kB 31.6MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 716kB 31.6MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 727kB 31.6MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 737kB 31.6MB/s eta 0:00:01\r\u001b[K     |██████████                      | 747kB 31.6MB/s eta 0:00:01\r\u001b[K     |██████████                      | 757kB 31.6MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 768kB 31.6MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 778kB 31.6MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 788kB 31.6MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 798kB 31.6MB/s eta 0:00:01\r\u001b[K     |██████████▊                     | 808kB 31.6MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 819kB 31.6MB/s eta 0:00:01\r\u001b[K     |███████████                     | 829kB 31.6MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 839kB 31.6MB/s eta 0:00:01\r\u001b[K     |███████████▎                    | 849kB 31.6MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 860kB 31.6MB/s eta 0:00:01\r\u001b[K     |███████████▌                    | 870kB 31.6MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 880kB 31.6MB/s eta 0:00:01\r\u001b[K     |███████████▉                    | 890kB 31.6MB/s eta 0:00:01\r\u001b[K     |████████████                    | 901kB 31.6MB/s eta 0:00:01\r\u001b[K     |████████████                    | 911kB 31.6MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 921kB 31.6MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 931kB 31.6MB/s eta 0:00:01\r\u001b[K     |████████████▌                   | 942kB 31.6MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 952kB 31.6MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 962kB 31.6MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 972kB 31.6MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 983kB 31.6MB/s eta 0:00:01\r\u001b[K     |█████████████▏                  | 993kB 31.6MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 1.0MB 31.6MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 1.0MB 31.6MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 1.0MB 31.6MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 1.0MB 31.6MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 1.0MB 31.6MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 1.1MB 31.6MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 1.1MB 31.6MB/s eta 0:00:01\r\u001b[K     |██████████████▎                 | 1.1MB 31.6MB/s eta 0:00:01\r\u001b[K     |██████████████▍                 | 1.1MB 31.6MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 1.1MB 31.6MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 1.1MB 31.6MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 1.1MB 31.6MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 1.1MB 31.6MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 1.1MB 31.6MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 1.1MB 31.6MB/s eta 0:00:01\r\u001b[K     |███████████████▎                | 1.2MB 31.6MB/s eta 0:00:01\r\u001b[K     |███████████████▌                | 1.2MB 31.6MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 1.2MB 31.6MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 1.2MB 31.6MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 1.2MB 31.6MB/s eta 0:00:01\r\u001b[K     |████████████████                | 1.2MB 31.6MB/s eta 0:00:01\r\u001b[K     |████████████████▏               | 1.2MB 31.6MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 1.2MB 31.6MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 1.2MB 31.6MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 1.2MB 31.6MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 1.3MB 31.6MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 1.3MB 31.6MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 1.3MB 31.6MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 1.3MB 31.6MB/s eta 0:00:01\r\u001b[K     |█████████████████▏              | 1.3MB 31.6MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 1.3MB 31.6MB/s eta 0:00:01\r\u001b[K     |█████████████████▌              | 1.3MB 31.6MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 1.3MB 31.6MB/s eta 0:00:01\r\u001b[K     |█████████████████▊              | 1.3MB 31.6MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 1.4MB 31.6MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 1.4MB 31.6MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 1.4MB 31.6MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 1.4MB 31.6MB/s eta 0:00:01\r\u001b[K     |██████████████████▍             | 1.4MB 31.6MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 1.4MB 31.6MB/s eta 0:00:01\r\u001b[K     |██████████████████▊             | 1.4MB 31.6MB/s eta 0:00:01\r\u001b[K     |██████████████████▉             | 1.4MB 31.6MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 1.4MB 31.6MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 1.4MB 31.6MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 1.5MB 31.6MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 1.5MB 31.6MB/s eta 0:00:01\r\u001b[K     |███████████████████▌            | 1.5MB 31.6MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 1.5MB 31.6MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 1.5MB 31.6MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 1.5MB 31.6MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 1.5MB 31.6MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 1.5MB 31.6MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 1.5MB 31.6MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 1.5MB 31.6MB/s eta 0:00:01\r\u001b[K     |████████████████████▋           | 1.6MB 31.6MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 1.6MB 31.6MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 1.6MB 31.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 1.6MB 31.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 1.6MB 31.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 1.6MB 31.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 1.6MB 31.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 1.6MB 31.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 1.6MB 31.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 1.6MB 31.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 1.7MB 31.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 1.7MB 31.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 1.7MB 31.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 1.7MB 31.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████▌         | 1.7MB 31.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 1.7MB 31.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 1.7MB 31.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 1.7MB 31.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 1.7MB 31.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 1.8MB 31.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████▎        | 1.8MB 31.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████▌        | 1.8MB 31.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 1.8MB 31.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 1.8MB 31.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 1.8MB 31.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 1.8MB 31.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 1.8MB 31.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 1.8MB 31.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 1.8MB 31.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████▌       | 1.9MB 31.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 1.9MB 31.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 1.9MB 31.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 1.9MB 31.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 1.9MB 31.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 1.9MB 31.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 1.9MB 31.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 1.9MB 31.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 1.9MB 31.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▊      | 1.9MB 31.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 2.0MB 31.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 2.0MB 31.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 2.0MB 31.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▎     | 2.0MB 31.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 2.0MB 31.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▋     | 2.0MB 31.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 2.0MB 31.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 2.0MB 31.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 2.0MB 31.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▏    | 2.0MB 31.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 2.1MB 31.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▍    | 2.1MB 31.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 2.1MB 31.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▋    | 2.1MB 31.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 2.1MB 31.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 2.1MB 31.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 2.1MB 31.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▏   | 2.1MB 31.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 2.1MB 31.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 2.2MB 31.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 2.2MB 31.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▊   | 2.2MB 31.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 2.2MB 31.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 2.2MB 31.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 2.2MB 31.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▎  | 2.2MB 31.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 2.2MB 31.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 2.2MB 31.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 2.2MB 31.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▉  | 2.3MB 31.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 2.3MB 31.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 2.3MB 31.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 2.3MB 31.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 2.3MB 31.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 2.3MB 31.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▋ | 2.3MB 31.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 2.3MB 31.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 2.3MB 31.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 2.3MB 31.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 2.4MB 31.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▎| 2.4MB 31.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 2.4MB 31.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 2.4MB 31.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 2.4MB 31.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 2.4MB 31.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 2.4MB 31.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging<21,>=20 in /usr/local/lib/python3.7/dist-packages (from tfx) (20.9)\n",
            "Requirement already satisfied: jinja2<3,>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from tfx) (2.11.3)\n",
            "Collecting tensorflow-data-validation<0.31,>=0.30\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d8/6b/a352a0077d902c94f4ab58e3f73d08e6b06a932b99ea8b6b385ac6d42686/tensorflow_data_validation-0.30.0-cp37-cp37m-manylinux2010_x86_64.whl (1.4MB)\n",
            "\u001b[K     |████████████████████████████████| 1.4MB 33.6MB/s \n",
            "\u001b[?25hCollecting tensorflow-hub<0.10,>=0.9.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ac/83/a7df82744a794107641dad1decaad017d82e25f0e1f761ac9204829eef96/tensorflow_hub-0.9.0-py2.py3-none-any.whl (103kB)\n",
            "\u001b[K     |████████████████████████████████| 112kB 56.8MB/s \n",
            "\u001b[?25hCollecting ml-metadata<0.31,>=0.30\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/dd/dc/08dab5572ae1a603d710c6b39629750abf16739a64fc8e331d0f378a8eef/ml_metadata-0.30.0-cp37-cp37m-manylinux2010_x86_64.whl (2.9MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9MB 39.5MB/s \n",
            "\u001b[?25hCollecting tensorflow-transform<0.31,>=0.30\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e6/b2/4b734ddf85d27cc28acd586a4d50cb677e8607db8d764dc8613424b20e29/tensorflow_transform-0.30.0-py3-none-any.whl (398kB)\n",
            "\u001b[K     |████████████████████████████████| 399kB 43.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: google-api-python-client<2,>=1.7.8 in /usr/local/lib/python3.7/dist-packages (from tfx) (1.12.8)\n",
            "Collecting kubernetes<12,>=10.0.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/59/46/d4b9364fcab59b5f9248e014c7592df8de84ad6548a6fe3de2d805bb75fc/kubernetes-11.0.0-py3-none-any.whl (1.5MB)\n",
            "\u001b[K     |████████████████████████████████| 1.5MB 29.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyyaml<6,>=3.12 in /usr/local/lib/python3.7/dist-packages (from tfx) (3.13)\n",
            "Collecting attrs<21,>=19.3.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c3/aa/cb45262569fcc047bf070b5de61813724d6726db83259222cd7b4c79821a/attrs-20.3.0-py2.py3-none-any.whl (49kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 5.7MB/s \n",
            "\u001b[?25hCollecting docker<5,>=4.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c4/22/410313ad554477e87ec406d38d85f810e61ddb0d2fc44e64994857476de9/docker-4.4.4-py2.py3-none-any.whl (147kB)\n",
            "\u001b[K     |████████████████████████████████| 153kB 30.0MB/s \n",
            "\u001b[?25hCollecting keras-tuner<1.0.2,>=1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a7/f7/4b41b6832abf4c9bef71a664dc563adb25afc5812831667c6db572b1a261/keras-tuner-1.0.1.tar.gz (54kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 7.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: portpicker<2,>=1.3.1 in /usr/local/lib/python3.7/dist-packages (from tfx) (1.3.9)\n",
            "Collecting apache-beam[gcp]<3,>=2.28\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ac/c9/395a9759dfbf9e87203a69c33b2e94f10d566d9391bddb6f99facafe64c3/apache_beam-2.30.0-cp37-cp37m-manylinux2010_x86_64.whl (9.6MB)\n",
            "\u001b[K     |████████████████████████████████| 9.6MB 28.6MB/s \n",
            "\u001b[?25hCollecting tensorflow-serving-api!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.5.*,<3,>=1.15\n",
            "  Downloading https://files.pythonhosted.org/packages/5f/a2/16169df906a0a342007ced647199e16163841fc4be7106a493ee42ef6a27/tensorflow_serving_api-2.4.1-py2.py3-none-any.whl\n",
            "Collecting tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.5.*,<3,>=1.15.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4e/dd/a6e880c0231416eb8ff51bf51e9b04cd08c600c01abc215f33f61cb23e6f/tensorflow-2.4.2-cp37-cp37m-manylinux2010_x86_64.whl (394.5MB)\n",
            "\u001b[K     |████████████████████████████████| 394.5MB 31kB/s \n",
            "\u001b[?25hCollecting tensorflow-model-analysis<0.31,>=0.30\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ab/89/afb019f92dbf78fe3d0981a6cfb8e5cc74f2595ef6d0c338e7feb85687da/tensorflow_model_analysis-0.30.0-py3-none-any.whl (1.7MB)\n",
            "\u001b[K     |████████████████████████████████| 1.7MB 25.9MB/s \n",
            "\u001b[?25hCollecting pyarrow<3,>=1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/2e/8d/c002e27767595f22aa09ed0d364327922f673d12b36526c967a2bf6b2ed7/pyarrow-2.0.0-cp37-cp37m-manylinux2014_x86_64.whl (17.7MB)\n",
            "\u001b[K     |████████████████████████████████| 17.7MB 201kB/s \n",
            "\u001b[?25hRequirement already satisfied: click<8,>=7 in /usr/local/lib/python3.7/dist-packages (from tfx) (7.1.2)\n",
            "Requirement already satisfied: protobuf<4,>=3.12.2 in /usr/local/lib/python3.7/dist-packages (from tfx) (3.12.4)\n",
            "Requirement already satisfied: grpcio<2,>=1.28.1 in /usr/local/lib/python3.7/dist-packages (from tfx) (1.34.1)\n",
            "Collecting tfx-bsl<0.31,>=0.30\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/53/0b/a3afbed67c44ce5d825f9c38529ce7f544973a10a44a4234331fc7463789/tfx_bsl-0.30.0-cp37-cp37m-manylinux2010_x86_64.whl (2.2MB)\n",
            "\u001b[K     |████████████████████████████████| 2.3MB 30.5MB/s \n",
            "\u001b[?25hCollecting google-cloud-aiplatform<0.8,>=0.5.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/2b/d3/7e702183e065ebfdb4dae899dfdb1d43c7525568c484314db579a4a39998/google_cloud_aiplatform-0.7.1-py2.py3-none-any.whl (1.3MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3MB 44.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: absl-py<0.13,>=0.9 in /usr/local/lib/python3.7/dist-packages (from tfx) (0.12.0)\n",
            "Collecting google-cloud-bigquery<3,>=1.28.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f6/4b/bbc0b268d50a8aec06aa8302b41f9d61c0c8603d16f25ddf7e9542d6b5a1/google_cloud_bigquery-2.20.0-py2.py3-none-any.whl (189kB)\n",
            "\u001b[K     |████████████████████████████████| 194kB 48.2MB/s \n",
            "\u001b[?25hCollecting ml-pipelines-sdk==0.30.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/76/10/978e3d5e95f164b8e8babf6a2653af47318256c9b896ac60ff2526045e76/ml_pipelines_sdk-0.30.1-py3-none-any.whl (1.2MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2MB 43.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy<1.20,>=1.16 in /usr/local/lib/python3.7/dist-packages (from tfx) (1.19.5)\n",
            "Requirement already satisfied: six<2,>=1.10 in /usr/local/lib/python3.7/dist-packages (from tfx) (1.15.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging<21,>=20->tfx) (2.4.7)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2<3,>=2.7.3->tfx) (2.0.1)\n",
            "Requirement already satisfied: pandas<2,>=1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-data-validation<0.31,>=0.30->tfx) (1.1.5)\n",
            "Collecting joblib<0.15,>=0.12\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/28/5c/cf6a2b65a321c4a209efcdf64c2689efae2cb62661f8f6f4bb28547cf1bf/joblib-0.14.1-py2.py3-none-any.whl (294kB)\n",
            "\u001b[K     |████████████████████████████████| 296kB 48.6MB/s \n",
            "\u001b[?25hCollecting tensorflow-metadata<0.31,>=0.30\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/27/e3/593025f6521de8b0f430b7c14e8adac6bae17002ca9a7841783065e56e20/tensorflow_metadata-0.30.0-py3-none-any.whl (47kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 6.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: pydot<2,>=1.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow-transform<0.31,>=0.30->tfx) (1.3.0)\n",
            "Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client<2,>=1.7.8->tfx) (3.0.1)\n",
            "Requirement already satisfied: google-api-core<2dev,>=1.21.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client<2,>=1.7.8->tfx) (1.26.3)\n",
            "Requirement already satisfied: google-auth>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client<2,>=1.7.8->tfx) (1.31.0)\n",
            "Requirement already satisfied: httplib2<1dev,>=0.15.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client<2,>=1.7.8->tfx) (0.17.4)\n",
            "Requirement already satisfied: google-auth-httplib2>=0.0.3 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client<2,>=1.7.8->tfx) (0.0.4)\n",
            "Requirement already satisfied: setuptools>=21.0.0 in /usr/local/lib/python3.7/dist-packages (from kubernetes<12,>=10.0.1->tfx) (57.0.0)\n",
            "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.7/dist-packages (from kubernetes<12,>=10.0.1->tfx) (1.3.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from kubernetes<12,>=10.0.1->tfx) (2.23.0)\n",
            "Requirement already satisfied: urllib3>=1.24.2 in /usr/local/lib/python3.7/dist-packages (from kubernetes<12,>=10.0.1->tfx) (1.24.3)\n",
            "Collecting websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ca/5f/3c211d168b2e9f9342cfb53bcfc26aab0eac63b998015e7af7bcae66119d/websocket_client-1.1.0-py2.py3-none-any.whl (68kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 8.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.7/dist-packages (from kubernetes<12,>=10.0.1->tfx) (2.8.1)\n",
            "Requirement already satisfied: certifi>=14.05.14 in /usr/local/lib/python3.7/dist-packages (from kubernetes<12,>=10.0.1->tfx) (2021.5.30)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from keras-tuner<1.0.2,>=1->tfx) (0.16.0)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.7/dist-packages (from keras-tuner<1.0.2,>=1->tfx) (0.8.9)\n",
            "Collecting terminaltables\n",
            "  Downloading https://files.pythonhosted.org/packages/9b/c4/4a21174f32f8a7e1104798c445dacdc1d4df86f2f26722767034e4de4bff/terminaltables-3.1.0.tar.gz\n",
            "Collecting colorama\n",
            "  Downloading https://files.pythonhosted.org/packages/44/98/5b86278fbbf250d239ae0ecb724f8572af1c91f4a11edf4d36a206189440/colorama-0.4.4-py2.py3-none-any.whl\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from keras-tuner<1.0.2,>=1->tfx) (4.41.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from keras-tuner<1.0.2,>=1->tfx) (1.4.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from keras-tuner<1.0.2,>=1->tfx) (0.22.2.post1)\n",
            "Requirement already satisfied: typing-extensions<3.8.0,>=3.7.0 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]<3,>=2.28->tfx) (3.7.4.3)\n",
            "Requirement already satisfied: pytz>=2018.3 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]<3,>=2.28->tfx) (2018.9)\n",
            "Requirement already satisfied: crcmod<2.0,>=1.7 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]<3,>=2.28->tfx) (1.7)\n",
            "Collecting fastavro<2,>=0.21.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/27/92/10ee74edb0a39f4a7af1cf271b3ac725c54f5c243c26fa5059cd794d15d7/fastavro-1.4.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.3MB)\n",
            "\u001b[K     |████████████████████████████████| 2.3MB 32.6MB/s \n",
            "\u001b[?25hCollecting dill<0.3.2,>=0.3.1.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c7/11/345f3173809cea7f1a193bfbf02403fff250a3360e0e118a1630985e547d/dill-0.3.1.1.tar.gz (151kB)\n",
            "\u001b[K     |████████████████████████████████| 153kB 53.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: pymongo<4.0.0,>=3.8.0 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]<3,>=2.28->tfx) (3.11.4)\n",
            "Collecting hdfs<3.0.0,>=2.1.0\n",
            "  Downloading https://files.pythonhosted.org/packages/08/f7/4c3fad73123a24d7394b6f40d1ec9c1cbf2e921cfea1797216ffd0a51fb1/hdfs-2.6.0-py3-none-any.whl\n",
            "Requirement already satisfied: oauth2client<5,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]<3,>=2.28->tfx) (4.1.3)\n",
            "Collecting avro-python3!=1.9.2,<1.10.0,>=1.8.1\n",
            "  Downloading https://files.pythonhosted.org/packages/5a/80/acd1455bea0a9fcdc60a748a97dcbb3ff624726fb90987a0fc1c19e7a5a5/avro-python3-1.9.2.1.tar.gz\n",
            "Collecting google-cloud-vision<2,>=0.38.0; extra == \"gcp\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0d/7f/e10d602c2dc3f749f1b78377a3357790f1da71b28e7da9e5bc20b3a9bd40/google_cloud_vision-1.0.0-py2.py3-none-any.whl (435kB)\n",
            "\u001b[K     |████████████████████████████████| 440kB 51.3MB/s \n",
            "\u001b[?25hCollecting google-apitools<0.5.32,>=0.5.31; extra == \"gcp\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/19/da/aefc4cf4c168b5d875344cd9dddc77e3a2d11986b630251af5ce47dd2843/google-apitools-0.5.31.tar.gz (173kB)\n",
            "\u001b[K     |████████████████████████████████| 174kB 55.6MB/s \n",
            "\u001b[?25hCollecting google-cloud-language<2,>=1.3.0; extra == \"gcp\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ba/b8/965a97ba60287910d342623da1da615254bded3e0965728cf7fc6339b7c8/google_cloud_language-1.3.0-py2.py3-none-any.whl (83kB)\n",
            "\u001b[K     |████████████████████████████████| 92kB 10.6MB/s \n",
            "\u001b[?25hCollecting google-cloud-dlp<2,>=0.12.0; extra == \"gcp\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9b/c3/5b73c15f59207b20df288573c2ea203c7b126df8330add380d8b50bc0d5c/google_cloud_dlp-1.0.0-py2.py3-none-any.whl (169kB)\n",
            "\u001b[K     |████████████████████████████████| 174kB 55.3MB/s \n",
            "\u001b[?25hCollecting grpcio-gcp<1,>=0.2.2; extra == \"gcp\"\n",
            "  Downloading https://files.pythonhosted.org/packages/ba/83/1f1095815be0de19102df41e250ebbd7dae97d7d14e22c18da07ed5ed9d4/grpcio_gcp-0.2.2-py2.py3-none-any.whl\n",
            "Requirement already satisfied: google-cloud-datastore<2,>=1.7.1; extra == \"gcp\" in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]<3,>=2.28->tfx) (1.8.0)\n",
            "Collecting google-cloud-spanner<2,>=1.13.0; extra == \"gcp\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/98/16/c9262ca40f3a278f38df9d21dece1ae01ee24f8ed29937bd1f066f908f9f/google_cloud_spanner-1.19.1-py2.py3-none-any.whl (255kB)\n",
            "\u001b[K     |████████████████████████████████| 256kB 56.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: google-cloud-core<2,>=0.28.1; extra == \"gcp\" in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]<3,>=2.28->tfx) (1.0.3)\n",
            "Collecting google-cloud-pubsub<2,>=0.39.0; extra == \"gcp\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1f/b3/dd83eca4cd1019d592e82595ea45d53f11e39db4ee99daa66ceb8a1b2d89/google_cloud_pubsub-1.7.0-py2.py3-none-any.whl (144kB)\n",
            "\u001b[K     |████████████████████████████████| 153kB 46.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: cachetools<5,>=3.1.0; extra == \"gcp\" in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]<3,>=2.28->tfx) (4.2.2)\n",
            "Collecting google-cloud-bigtable<2,>=0.31.1; extra == \"gcp\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/2a/17/536768bada8f93f124826b36dbdcdf08edd0e5ef0ca76b4c911f9f28596a/google_cloud_bigtable-1.7.0-py2.py3-none-any.whl (267kB)\n",
            "\u001b[K     |████████████████████████████████| 276kB 45.9MB/s \n",
            "\u001b[?25hCollecting google-cloud-videointelligence<2,>=1.8.0; extra == \"gcp\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e1/29/8d06211102c87768dc34943d9c92abd8b67491ffedd6d09b56305b1ab255/google_cloud_videointelligence-1.16.1-py2.py3-none-any.whl (183kB)\n",
            "\u001b[K     |████████████████████████████████| 184kB 52.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: termcolor~=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.5.*,<3,>=1.15.2->tfx) (1.1.0)\n",
            "Collecting gast==0.3.3\n",
            "  Downloading https://files.pythonhosted.org/packages/d6/84/759f5dd23fec8ba71952d97bcc7e2c9d7d63bdc582421f3cd4be845f0c98/gast-0.3.3-py2.py3-none-any.whl\n",
            "Requirement already satisfied: flatbuffers~=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.5.*,<3,>=1.15.2->tfx) (1.12)\n",
            "Requirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.7/dist-packages (from tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.5.*,<3,>=1.15.2->tfx) (0.36.2)\n",
            "Requirement already satisfied: wrapt~=1.12.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.5.*,<3,>=1.15.2->tfx) (1.12.1)\n",
            "Requirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.5.*,<3,>=1.15.2->tfx) (0.2.0)\n",
            "Requirement already satisfied: tensorboard~=2.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.5.*,<3,>=1.15.2->tfx) (2.5.0)\n",
            "Requirement already satisfied: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.5.*,<3,>=1.15.2->tfx) (1.1.2)\n",
            "Collecting h5py~=2.10.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3f/c0/abde58b837e066bca19a3f7332d9d0493521d7dd6b48248451a9e3fe2214/h5py-2.10.0-cp37-cp37m-manylinux1_x86_64.whl (2.9MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9MB 21.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: opt-einsum~=3.3.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.5.*,<3,>=1.15.2->tfx) (3.3.0)\n",
            "Requirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.5.*,<3,>=1.15.2->tfx) (1.6.3)\n",
            "Collecting tensorflow-estimator<2.5.0,>=2.4.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/74/7e/622d9849abf3afb81e482ffc170758742e392ee129ce1540611199a59237/tensorflow_estimator-2.4.0-py2.py3-none-any.whl (462kB)\n",
            "\u001b[K     |████████████████████████████████| 471kB 34.3MB/s \n",
            "\u001b[?25hCollecting ipython<8,>=7\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f7/b2/733ea4551a04866bbcfbbade4d9d2c82c829cf1cc6fac1ac5974b8c7f756/ipython-7.25.0-py3-none-any.whl (786kB)\n",
            "\u001b[K     |████████████████████████████████| 788kB 45.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: ipywidgets<8,>=7 in /usr/local/lib/python3.7/dist-packages (from tensorflow-model-analysis<0.31,>=0.30->tfx) (7.6.3)\n",
            "Collecting google-cloud-storage<2.0.0dev,>=1.26.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/92/b7/10f7086520bc7f4e80e10dce9c41f5317e7c6a7e1234bf2e1fe7c7193130/google_cloud_storage-1.39.0-py2.py3-none-any.whl (103kB)\n",
            "\u001b[K     |████████████████████████████████| 112kB 47.5MB/s \n",
            "\u001b[?25hCollecting proto-plus>=1.10.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8c/72/6f3f4cdc5bb0294f8d7f3f8aacb617b4c3cb17554ed78f7e28009162c795/proto_plus-1.19.0-py3-none-any.whl (42kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 4.2MB/s \n",
            "\u001b[?25hCollecting google-resumable-media<2.0dev,>=0.6.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/11/96/4360dc70bef5559b3faf3deeda97aae7d10ff7660d41fd233eb792e7d09f/google_resumable_media-1.3.1-py2.py3-none-any.whl (75kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 10.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: googleapis-common-protos<2,>=1.52.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-metadata<0.31,>=0.30->tensorflow-data-validation<0.31,>=0.30->tfx) (1.53.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth>=1.16.0->google-api-python-client<2,>=1.7.8->tfx) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from google-auth>=1.16.0->google-api-python-client<2,>=1.7.8->tfx) (4.7.2)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib->kubernetes<12,>=10.0.1->tfx) (3.1.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->kubernetes<12,>=10.0.1->tfx) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->kubernetes<12,>=10.0.1->tfx) (3.0.4)\n",
            "Requirement already satisfied: docopt in /usr/local/lib/python3.7/dist-packages (from hdfs<3.0.0,>=2.1.0->apache-beam[gcp]<3,>=2.28->tfx) (0.6.2)\n",
            "Requirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.7/dist-packages (from oauth2client<5,>=2.0.1->apache-beam[gcp]<3,>=2.28->tfx) (0.4.8)\n",
            "Collecting fasteners>=0.14\n",
            "  Downloading https://files.pythonhosted.org/packages/31/91/6630ebd169ca170634ca8a10dfcc5f5c11b0621672d4c2c9e40381c6d81a/fasteners-0.16.3-py2.py3-none-any.whl\n",
            "Collecting grpc-google-iam-v1<0.13dev,>=0.12.3\n",
            "  Downloading https://files.pythonhosted.org/packages/65/19/2060c8faa325fddc09aa67af98ffcb6813f39a0ad805679fa64815362b3a/grpc-google-iam-v1-0.12.3.tar.gz\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.5.*,<3,>=1.15.2->tfx) (0.4.4)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.5.*,<3,>=1.15.2->tfx) (0.6.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.5.*,<3,>=1.15.2->tfx) (3.3.4)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.5.*,<3,>=1.15.2->tfx) (1.8.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.5.*,<3,>=1.15.2->tfx) (1.0.1)\n",
            "Collecting prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1e/34/1d9880ac1339ad4c6697b330e7a507584105613751318249d9e820faa25f/prompt_toolkit-3.0.19-py3-none-any.whl (368kB)\n",
            "\u001b[K     |████████████████████████████████| 368kB 42.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython<8,>=7->tensorflow-model-analysis<0.31,>=0.30->tfx) (4.4.2)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.7/dist-packages (from ipython<8,>=7->tensorflow-model-analysis<0.31,>=0.30->tfx) (0.1.2)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython<8,>=7->tensorflow-model-analysis<0.31,>=0.30->tfx) (2.6.1)\n",
            "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.7/dist-packages (from ipython<8,>=7->tensorflow-model-analysis<0.31,>=0.30->tfx) (0.18.0)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.7/dist-packages (from ipython<8,>=7->tensorflow-model-analysis<0.31,>=0.30->tfx) (5.0.5)\n",
            "Requirement already satisfied: pexpect>4.3; sys_platform != \"win32\" in /usr/local/lib/python3.7/dist-packages (from ipython<8,>=7->tensorflow-model-analysis<0.31,>=0.30->tfx) (4.8.0)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.7/dist-packages (from ipython<8,>=7->tensorflow-model-analysis<0.31,>=0.30->tfx) (0.2.0)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython<8,>=7->tensorflow-model-analysis<0.31,>=0.30->tfx) (0.7.5)\n",
            "Requirement already satisfied: widgetsnbextension~=3.5.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets<8,>=7->tensorflow-model-analysis<0.31,>=0.30->tfx) (3.5.1)\n",
            "Requirement already satisfied: nbformat>=4.2.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets<8,>=7->tensorflow-model-analysis<0.31,>=0.30->tfx) (5.1.3)\n",
            "Requirement already satisfied: ipykernel>=4.5.1 in /usr/local/lib/python3.7/dist-packages (from ipywidgets<8,>=7->tensorflow-model-analysis<0.31,>=0.30->tfx) (4.10.1)\n",
            "Requirement already satisfied: jupyterlab-widgets>=1.0.0; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from ipywidgets<8,>=7->tensorflow-model-analysis<0.31,>=0.30->tfx) (1.0.0)\n",
            "Collecting google-crc32c<2.0dev,>=1.0; python_version >= \"3.5\"\n",
            "  Downloading https://files.pythonhosted.org/packages/fc/ae/b6efa1019e18c6c791f0f5cd93b2ff40f8f06696dbf04db39ec0f5591b1e/google_crc32c-1.1.2-cp37-cp37m-manylinux2014_x86_64.whl\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard~=2.4->tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.5.*,<3,>=1.15.2->tfx) (4.5.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython<8,>=7->tensorflow-model-analysis<0.31,>=0.30->tfx) (0.2.5)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from jedi>=0.16->ipython<8,>=7->tensorflow-model-analysis<0.31,>=0.30->tfx) (0.8.2)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.7/dist-packages (from traitlets>=4.2->ipython<8,>=7->tensorflow-model-analysis<0.31,>=0.30->tfx) (0.2.0)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect>4.3; sys_platform != \"win32\"->ipython<8,>=7->tensorflow-model-analysis<0.31,>=0.30->tfx) (0.7.0)\n",
            "Requirement already satisfied: notebook>=4.4.1 in /usr/local/lib/python3.7/dist-packages (from widgetsnbextension~=3.5.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.31,>=0.30->tfx) (5.3.1)\n",
            "Requirement already satisfied: jupyter-core in /usr/local/lib/python3.7/dist-packages (from nbformat>=4.2.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.31,>=0.30->tfx) (4.7.1)\n",
            "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /usr/local/lib/python3.7/dist-packages (from nbformat>=4.2.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.31,>=0.30->tfx) (2.6.0)\n",
            "Requirement already satisfied: tornado>=4.0 in /usr/local/lib/python3.7/dist-packages (from ipykernel>=4.5.1->ipywidgets<8,>=7->tensorflow-model-analysis<0.31,>=0.30->tfx) (5.1.1)\n",
            "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.7/dist-packages (from ipykernel>=4.5.1->ipywidgets<8,>=7->tensorflow-model-analysis<0.31,>=0.30->tfx) (5.3.5)\n",
            "Requirement already satisfied: cffi>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from google-crc32c<2.0dev,>=1.0; python_version >= \"3.5\"->google-resumable-media<2.0dev,>=0.6.0->google-cloud-bigquery<3,>=1.28.0->tfx) (1.14.5)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard~=2.4->tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.5.*,<3,>=1.15.2->tfx) (3.4.1)\n",
            "Requirement already satisfied: nbconvert in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.31,>=0.30->tfx) (5.6.1)\n",
            "Requirement already satisfied: Send2Trash in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.31,>=0.30->tfx) (1.5.0)\n",
            "Requirement already satisfied: terminado>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.31,>=0.30->tfx) (0.10.1)\n",
            "Requirement already satisfied: pyzmq>=13 in /usr/local/lib/python3.7/dist-packages (from jupyter-client->ipykernel>=4.5.1->ipywidgets<8,>=7->tensorflow-model-analysis<0.31,>=0.30->tfx) (22.1.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.0.0->google-crc32c<2.0dev,>=1.0; python_version >= \"3.5\"->google-resumable-media<2.0dev,>=0.6.0->google-cloud-bigquery<3,>=1.28.0->tfx) (2.20)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.31,>=0.30->tfx) (0.7.1)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.31,>=0.30->tfx) (0.8.4)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.31,>=0.30->tfx) (3.3.0)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.31,>=0.30->tfx) (1.4.3)\n",
            "Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.31,>=0.30->tfx) (0.3)\n",
            "Requirement already satisfied: testpath in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.31,>=0.30->tfx) (0.5.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.7/dist-packages (from bleach->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.31,>=0.30->tfx) (0.5.1)\n",
            "Building wheels for collected packages: keras-tuner, terminaltables, dill, avro-python3, google-apitools, grpc-google-iam-v1\n",
            "  Building wheel for keras-tuner (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-tuner: filename=keras_tuner-1.0.1-cp37-none-any.whl size=73199 sha256=359f4db8130ac5e199bd93a66e697d5b2176c6f33354f23d7d4bd3b7ef878af5\n",
            "  Stored in directory: /root/.cache/pip/wheels/b9/cc/62/52716b70dd90f3db12519233c3a93a5360bc672da1a10ded43\n",
            "  Building wheel for terminaltables (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for terminaltables: filename=terminaltables-3.1.0-cp37-none-any.whl size=15356 sha256=200559a8c63916cbe5ebfedfcdc67c3bd7f3f87fd197e545d273a7de945da604\n",
            "  Stored in directory: /root/.cache/pip/wheels/30/6b/50/6c75775b681fb36cdfac7f19799888ef9d8813aff9e379663e\n",
            "  Building wheel for dill (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for dill: filename=dill-0.3.1.1-cp37-none-any.whl size=78545 sha256=cc6cc984ab28e162775f29237cb1149d6a303e615558541de6b7e7c6d9434055\n",
            "  Stored in directory: /root/.cache/pip/wheels/59/b1/91/f02e76c732915c4015ab4010f3015469866c1eb9b14058d8e7\n",
            "  Building wheel for avro-python3 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for avro-python3: filename=avro_python3-1.9.2.1-cp37-none-any.whl size=43516 sha256=1a3356e61c136db7cbb28e61d041d93014b3a48a65604aed146739a54be3d9fb\n",
            "  Stored in directory: /root/.cache/pip/wheels/5a/d3/be/86620c9dd3fca68986c33b9c616510289fc0abb81ec9aa70bd\n",
            "  Building wheel for google-apitools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for google-apitools: filename=google_apitools-0.5.31-cp37-none-any.whl size=131043 sha256=0ef1fe5ddedbcf5f669c38276cefcf222c771a9f8791e269286e9c6650255fa8\n",
            "  Stored in directory: /root/.cache/pip/wheels/3b/43/31/09a9dad88d3aec6fed2d63bd35dfc532fca372e2edec5af5bf\n",
            "  Building wheel for grpc-google-iam-v1 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for grpc-google-iam-v1: filename=grpc_google_iam_v1-0.12.3-cp37-none-any.whl size=18516 sha256=d1baed057ca6a7d433b8eb779a6cf78c399c45b5dad5a42a4fd20a0b7f0ce63d\n",
            "  Stored in directory: /root/.cache/pip/wheels/de/3a/83/77a1e18e1a8757186df834b86ce6800120ac9c79cd8ca4091b\n",
            "Successfully built keras-tuner terminaltables dill avro-python3 google-apitools grpc-google-iam-v1\n",
            "\u001b[31mERROR: tensorflow 2.4.2 has requirement grpcio~=1.32.0, but you'll have grpcio 1.34.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: pandas-gbq 0.13.3 has requirement google-cloud-bigquery[bqstorage,pandas]<2.0.0dev,>=1.11.1, but you'll have google-cloud-bigquery 2.20.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: multiprocess 0.70.12.2 has requirement dill>=0.3.4, but you'll have dill 0.3.1.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: jupyter-console 5.2.0 has requirement prompt-toolkit<2.0.0,>=1.0.0, but you'll have prompt-toolkit 3.0.19 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: google-colab 1.0.0 has requirement ipython~=5.5.0, but you'll have ipython 7.25.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: google-cloud-storage 1.39.0 has requirement google-cloud-core<2.0dev,>=1.4.1, but you'll have google-cloud-core 1.0.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: google-cloud-bigquery 2.20.0 has requirement google-api-core[grpc]<2.0.0dev,>=1.29.0, but you'll have google-api-core 1.26.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: google-cloud-bigquery 2.20.0 has requirement google-cloud-core<2.0dev,>=1.4.1, but you'll have google-cloud-core 1.0.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: google-cloud-spanner 1.19.1 has requirement google-cloud-core<2.0dev,>=1.4.1, but you'll have google-cloud-core 1.0.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: google-cloud-bigtable 1.7.0 has requirement google-cloud-core<2.0dev,>=1.4.1, but you'll have google-cloud-core 1.0.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: apache-beam 2.30.0 has requirement future<1.0.0,>=0.18.2, but you'll have future 0.16.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: apache-beam 2.30.0 has requirement requests<3.0.0,>=2.24.0, but you'll have requests 2.23.0 which is incompatible.\u001b[0m\n",
            "Installing collected packages: pyarrow, gast, h5py, tensorflow-estimator, tensorflow, tensorflow-serving-api, tensorflow-metadata, fastavro, dill, hdfs, avro-python3, google-cloud-vision, fasteners, google-apitools, google-cloud-language, google-cloud-dlp, grpcio-gcp, grpc-google-iam-v1, google-cloud-spanner, google-crc32c, google-resumable-media, proto-plus, google-cloud-bigquery, google-cloud-pubsub, google-cloud-bigtable, google-cloud-videointelligence, apache-beam, tfx-bsl, joblib, tensorflow-data-validation, tensorflow-hub, attrs, ml-metadata, tensorflow-transform, websocket-client, kubernetes, docker, terminaltables, colorama, keras-tuner, prompt-toolkit, ipython, tensorflow-model-analysis, google-cloud-storage, google-cloud-aiplatform, ml-pipelines-sdk, tfx\n",
            "  Found existing installation: pyarrow 3.0.0\n",
            "    Uninstalling pyarrow-3.0.0:\n",
            "      Successfully uninstalled pyarrow-3.0.0\n",
            "  Found existing installation: gast 0.4.0\n",
            "    Uninstalling gast-0.4.0:\n",
            "      Successfully uninstalled gast-0.4.0\n",
            "  Found existing installation: h5py 3.1.0\n",
            "    Uninstalling h5py-3.1.0:\n",
            "      Successfully uninstalled h5py-3.1.0\n",
            "  Found existing installation: tensorflow-estimator 2.5.0\n",
            "    Uninstalling tensorflow-estimator-2.5.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.5.0\n",
            "  Found existing installation: tensorflow 2.5.0\n",
            "    Uninstalling tensorflow-2.5.0:\n",
            "      Successfully uninstalled tensorflow-2.5.0\n",
            "  Found existing installation: tensorflow-metadata 1.0.0\n",
            "    Uninstalling tensorflow-metadata-1.0.0:\n",
            "      Successfully uninstalled tensorflow-metadata-1.0.0\n",
            "  Found existing installation: dill 0.3.4\n",
            "    Uninstalling dill-0.3.4:\n",
            "      Successfully uninstalled dill-0.3.4\n",
            "  Found existing installation: google-cloud-language 1.2.0\n",
            "    Uninstalling google-cloud-language-1.2.0:\n",
            "      Successfully uninstalled google-cloud-language-1.2.0\n",
            "  Found existing installation: google-resumable-media 0.4.1\n",
            "    Uninstalling google-resumable-media-0.4.1:\n",
            "      Successfully uninstalled google-resumable-media-0.4.1\n",
            "  Found existing installation: google-cloud-bigquery 1.21.0\n",
            "    Uninstalling google-cloud-bigquery-1.21.0:\n",
            "      Successfully uninstalled google-cloud-bigquery-1.21.0\n",
            "  Found existing installation: joblib 1.0.1\n",
            "    Uninstalling joblib-1.0.1:\n",
            "      Successfully uninstalled joblib-1.0.1\n",
            "  Found existing installation: tensorflow-hub 0.12.0\n",
            "    Uninstalling tensorflow-hub-0.12.0:\n",
            "      Successfully uninstalled tensorflow-hub-0.12.0\n",
            "  Found existing installation: attrs 21.2.0\n",
            "    Uninstalling attrs-21.2.0:\n",
            "      Successfully uninstalled attrs-21.2.0\n",
            "  Found existing installation: prompt-toolkit 1.0.18\n",
            "    Uninstalling prompt-toolkit-1.0.18:\n",
            "      Successfully uninstalled prompt-toolkit-1.0.18\n",
            "  Found existing installation: ipython 5.5.0\n",
            "    Uninstalling ipython-5.5.0:\n",
            "      Successfully uninstalled ipython-5.5.0\n",
            "  Found existing installation: google-cloud-storage 1.18.1\n",
            "    Uninstalling google-cloud-storage-1.18.1:\n",
            "      Successfully uninstalled google-cloud-storage-1.18.1\n",
            "Successfully installed apache-beam-2.30.0 attrs-20.3.0 avro-python3-1.9.2.1 colorama-0.4.4 dill-0.3.1.1 docker-4.4.4 fastavro-1.4.2 fasteners-0.16.3 gast-0.3.3 google-apitools-0.5.31 google-cloud-aiplatform-0.7.1 google-cloud-bigquery-2.20.0 google-cloud-bigtable-1.7.0 google-cloud-dlp-1.0.0 google-cloud-language-1.3.0 google-cloud-pubsub-1.7.0 google-cloud-spanner-1.19.1 google-cloud-storage-1.39.0 google-cloud-videointelligence-1.16.1 google-cloud-vision-1.0.0 google-crc32c-1.1.2 google-resumable-media-1.3.1 grpc-google-iam-v1-0.12.3 grpcio-gcp-0.2.2 h5py-2.10.0 hdfs-2.6.0 ipython-7.25.0 joblib-0.14.1 keras-tuner-1.0.1 kubernetes-11.0.0 ml-metadata-0.30.0 ml-pipelines-sdk-0.30.1 prompt-toolkit-3.0.19 proto-plus-1.19.0 pyarrow-2.0.0 tensorflow-2.4.2 tensorflow-data-validation-0.30.0 tensorflow-estimator-2.4.0 tensorflow-hub-0.9.0 tensorflow-metadata-0.30.0 tensorflow-model-analysis-0.30.0 tensorflow-serving-api-2.4.1 tensorflow-transform-0.30.0 terminaltables-3.1.0 tfx-0.30.1 tfx-bsl-0.30.0 websocket-client-1.1.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "IPython",
                  "dill",
                  "gast",
                  "google",
                  "h5py",
                  "prompt_toolkit",
                  "tensorflow"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YIqpWK9efviJ"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "from tfx.components import CsvExampleGen\n",
        "from tfx.components import ExampleValidator\n",
        "from tfx.components import SchemaGen\n",
        "from tfx.components import StatisticsGen\n",
        "from tfx.components import Transform\n",
        "\n",
        "from tfx.orchestration.experimental.interactive.interactive_context import InteractiveContext\n",
        "from google.protobuf.json_format import MessageToDict\n",
        "\n",
        "import os\n",
        "import pprint\n",
        "\n",
        "pp = pprint.PrettyPrinter()"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lavaw2XsrIXf"
      },
      "source": [
        "<a name='1-2'></a>\n",
        "### 1.2 - Define paths\n",
        "\n",
        "You will define a few global variables to indicate paths in the local workspace."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fIpszRKss_r0",
        "outputId": "e8e570d2-2267-4a82-f506-0c7d587553e3",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 73
        }
      },
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-d931eef9-5765-423d-bb74-ad2f962ad179\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-d931eef9-5765-423d-bb74-ad2f962ad179\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving metro_traffic_volume.csv to metro_traffic_volume.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tvXEVqR1tHMq",
        "outputId": "aac72dd8-2ada-4ff9-dd2c-e74ca645209b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "! ls"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "metro_traffic_volume.csv  sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pu5lkXvQt0m7"
      },
      "source": [
        "! mkdir metro_traffic_pipeline"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Cz-HqRZt_nU"
      },
      "source": [
        "! mkdir metro_traffic_pipeline/data"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nFSUqKC4tnzi"
      },
      "source": [
        "!mv \"metro_traffic_volume.csv\" \"metro_traffic_pipeline/data/\""
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pJAGLDLwuCMz",
        "outputId": "c5aa4b5e-3283-4863-c23f-5a39de822dff",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!ls metro_traffic_pipeline/data"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "metro_traffic_volume.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aekkb_Y8rIXf"
      },
      "source": [
        "# location of the pipeline metadata store\n",
        "_pipeline_root = 'metro_traffic_pipeline/'\n",
        "\n",
        "# directory of the raw data files\n",
        "_data_root = 'metro_traffic_pipeline/data'\n",
        "\n",
        "# path to the raw training data\n",
        "_data_filepath = os.path.join(_data_root, 'metro_traffic_volume.csv')"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n2cMMAbSkGfX"
      },
      "source": [
        "<a name='1-3'></a>\n",
        "### 1.3 - Preview the  dataset\n",
        "\n",
        "The [Metro Interstate Traffic Volume dataset](https://archive.ics.uci.edu/ml/datasets/Metro+Interstate+Traffic+Volume) contains hourly traffic volume of a road in Minnesota from 2012-2018. With this data, you can develop a model for predicting the traffic volume given the date, time, and weather conditions. The attributes are:\n",
        "\n",
        "* **holiday** - US National holidays plus regional holiday, Minnesota State Fair\n",
        "* **temp** - Average temp in Kelvin\n",
        "* **rain_1h** - Amount in mm of rain that occurred in the hour\n",
        "* **snow_1h** - Amount in mm of snow that occurred in the hour\n",
        "* **clouds_all** - Percentage of cloud cover\n",
        "* **weather_main** - Short textual description of the current weather\n",
        "* **weather_description** - Longer textual description of the current weather\n",
        "* **date_time** - DateTime Hour of the data collected in local CST time\n",
        "* **traffic_volume** - Numeric Hourly I-94 ATR 301 reported westbound traffic volume\n",
        "* **month** - taken from date_time\n",
        "* **day** - taken from date_time\n",
        "* **day_of_week** - taken from date_time\n",
        "* **hour** - taken from date_time\n",
        "\n",
        "\n",
        "*Disclaimer: We added the last four attributes shown above (i.e. month, day, day_of_week, hour) to the original dataset to increase the features you can transform later.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "blZC1sIQOWfH"
      },
      "source": [
        "Take a quick look at the first few rows of the CSV file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c5YPeLPFOXaD",
        "outputId": "58442f3a-2732-4237-b474-0298df9730b9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Preview the dataset\n",
        "!head {_data_filepath}"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "holiday,temp,rain_1h,snow_1h,clouds_all,weather_main,weather_description,date_time,traffic_volume,month,day,day_of_week,hour\n",
            "None,288.28,0.0,0.0,40,Clouds,scattered clouds,2012-10-02 09:00:00,5545,10,2,1,9\n",
            "None,289.36,0.0,0.0,75,Clouds,broken clouds,2012-10-02 10:00:00,4516,10,2,1,10\n",
            "None,289.58,0.0,0.0,90,Clouds,overcast clouds,2012-10-02 11:00:00,4767,10,2,1,11\n",
            "None,290.13,0.0,0.0,90,Clouds,overcast clouds,2012-10-02 12:00:00,5026,10,2,1,12\n",
            "None,291.14,0.0,0.0,75,Clouds,broken clouds,2012-10-02 13:00:00,4918,10,2,1,13\n",
            "None,291.72,0.0,0.0,1,Clear,sky is clear,2012-10-02 14:00:00,5181,10,2,1,14\n",
            "None,293.17,0.0,0.0,1,Clear,sky is clear,2012-10-02 15:00:00,5584,10,2,1,15\n",
            "None,293.86,0.0,0.0,1,Clear,sky is clear,2012-10-02 16:00:00,6015,10,2,1,16\n",
            "None,294.14,0.0,0.0,20,Clouds,few clouds,2012-10-02 17:00:00,5791,10,2,1,17\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ONIE_hdkPS4"
      },
      "source": [
        "<a name='1-4'></a>\n",
        "### 1.4 - Create the InteractiveContext\n",
        "\n",
        "You will need to initialize the `InteractiveContext` to enable running the TFX components interactively. As before, you will let it create the metadata store in the `_pipeline_root` directory. You can safely ignore the warning about the missing metadata config file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Rh6K5sUf9dd",
        "outputId": "7988f18b-d41e-4a97-8a52-b3b6547a202e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Declare the InteractiveContext and use a local sqlite file as the metadata store.\n",
        "# You can ignore the warning about the missing metadata config file\n",
        "context = InteractiveContext(pipeline_root=_pipeline_root)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:InteractiveContext metadata_connection_config not provided: using SQLite ML Metadata database at metro_traffic_pipeline/metadata.sqlite.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HdQWxfsVkzdJ"
      },
      "source": [
        "<a name='2'></a>\n",
        "## 2 - Run TFX components interactively\n",
        "\n",
        "In the following exercises, you will create the data pipeline components one-by-one, run each of them, and visualize their output artifacts. Recall that we refer to the outputs of pipeline components as *artifacts* and these can be inputs to the next stage of the pipeline."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L9fwt9gQk3BR"
      },
      "source": [
        "<a name='2-1'></a>\n",
        "### 2.1 - ExampleGen\n",
        "\n",
        "The pipeline starts with the [ExampleGen](https://www.tensorflow.org/tfx/guide/examplegen) component. It will:\n",
        "\n",
        "*   split the data into training and evaluation sets (by default: 2/3 train, 1/3 eval).\n",
        "*   convert each data row into `tf.train.Example` format. This [protocol buffer](https://developers.google.com/protocol-buffers) is designed for Tensorflow operations and is used by the TFX components.\n",
        "*   compress and save the data collection under the `_pipeline_root` directory for other components to access. These examples are stored in `TFRecord` format. This optimizes read and write operations within Tensorflow especially if you have a large collection of data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sfhWjzfxrIXj"
      },
      "source": [
        "<a name='ex-1'></a>\n",
        "#### Exercise 1: ExampleGen\n",
        "\n",
        "Fill out the code below to ingest the data from the CSV file stored in the `_data_root` directory."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PyXjuMt8f-9u",
        "outputId": "166443ff-1c6d-4a3a-bc83-7a1e87c983b8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "### START CODE HERE\n",
        "\n",
        "# Instantiate ExampleGen with the input CSV dataset\n",
        "example_gen = CsvExampleGen(input_base=_data_root)\n",
        "\n",
        "# Run the component using the InteractiveContext instance\n",
        "context = InteractiveContext(pipeline_root=_pipeline_root)\n",
        "\n",
        "### END CODE HERE"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:InteractiveContext metadata_connection_config not provided: using SQLite ML Metadata database at metro_traffic_pipeline/metadata.sqlite.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OqCoZh7KPUm9"
      },
      "source": [
        "You should see the output cell of the `InteractiveContext` above showing the metadata associated with the component execution. You can expand the items under `.component.outputs` and see that an `Examples` artifact for the train and eval split is created in `metro_traffic_pipeline/CsvExampleGen/examples/{execution_id}`. \n",
        "\n",
        "You can also check that programmatically with the following snippet. You can focus on the `try` block. The `except` and `else` block is needed mainly for grading. `context.run()` yields no operation when executed in a non-interactive environment (such as the grader script that runs outside of this notebook). In such scenarios, the URI must be manually set to avoid errors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "880KkTAkPeUg",
        "outputId": "63871ce3-0401-4e26-c14c-f5e5b99d421e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 415
        }
      },
      "source": [
        "try:\n",
        "    # get the artifact object\n",
        "    artifact = example_gen.outputs['examples'].get()[0]\n",
        "    \n",
        "    # print split names and uri\n",
        "    print(f'split names: {artifact.split_names}')\n",
        "    print(f'artifact uri: {artifact.uri}')\n",
        "\n",
        "# for grading since context.run() does not work outside the notebook\n",
        "except IndexError:\n",
        "    print(\"context.run() was no-op\")\n",
        "    examples_path = './metro_traffic_pipeline/CsvExampleGen/examples'\n",
        "    dir_id = os.listdir(examples_path)[0]\n",
        "    artifact_uri = f'{examples_path}/{dir_id}'\n",
        "\n",
        "else:\n",
        "    artifact_uri = artifact.uri"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "context.run() was no-op\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-edd812b8ab4b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;31m# get the artifact object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0martifact\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexample_gen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'examples'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: list index out of range",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-edd812b8ab4b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"context.run() was no-op\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mexamples_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'./metro_traffic_pipeline/CsvExampleGen/examples'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mdir_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexamples_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0martifact_uri\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf'{examples_path}/{dir_id}'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './metro_traffic_pipeline/CsvExampleGen/examples'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J6vcbW_wPqvl"
      },
      "source": [
        "The ingested data has been saved to the directory specified by the artifact Uniform Resource Identifier (URI). As a sanity check, you can take a look at some of the training examples. This requires working with Tensorflow data types, particularly `tf.train.Example` and `TFRecord` (you can read more about them [here](https://www.tensorflow.org/tutorials/load_data/tfrecord)). Let's first load the `TFRecord` into a variable:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H4XIXjiCPwzQ"
      },
      "source": [
        "# Get the URI of the output artifact representing the training examples, which is a directory\n",
        "train_uri = os.path.join(artifact_uri, 'train')\n",
        "\n",
        "# Get the list of files in this directory (all compressed TFRecord files)\n",
        "tfrecord_filenames = [os.path.join(train_uri, name)\n",
        "                      for name in os.listdir(train_uri)]\n",
        "\n",
        "# Create a `TFRecordDataset` to read these files\n",
        "dataset = tf.data.TFRecordDataset(tfrecord_filenames, compression_type=\"GZIP\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yq6BjvczrIXl"
      },
      "source": [
        "<a name='ex-2'></a>\n",
        "#### Exercise 2: get_records()\n",
        "\n",
        "Complete the helper function below to return a specified number of examples.\n",
        "\n",
        "*Hints: You may find the [MessageToDict](https://googleapis.dev/python/protobuf/latest/google/protobuf/json_format.html#google.protobuf.json_format.MessageToDict) helper function and tf.train.Example's [ParseFromString()](https://googleapis.dev/python/protobuf/latest/google/protobuf/message.html#google.protobuf.message.Message.ParseFromString) method useful here. You can also refer [here](https://www.tensorflow.org/tutorials/load_data/tfrecord) for a refresher on TFRecord and tf.train.Example()*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z70bJac6rIXl"
      },
      "source": [
        "def get_records(dataset, num_records):\n",
        "    '''Extracts records from the given dataset.\n",
        "    Args:\n",
        "        dataset (TFRecordDataset): dataset saved by ExampleGen\n",
        "        num_records (int): number of records to preview\n",
        "    '''\n",
        "    \n",
        "    # initialize an empty list\n",
        "    records = []\n",
        "\n",
        "    ### START CODE HERE\n",
        "    # Use the `take()` method to specify how many records to get\n",
        "    for tfrecord in dataset.take(num_records):\n",
        "        \n",
        "        # Get the numpy property of the tensor\n",
        "        serialized_example = None\n",
        "        \n",
        "        # Initialize a `tf.train.Example()` to read the serialized data\n",
        "        example = None\n",
        "        \n",
        "        # Read the example data (output is a protocol buffer message)\n",
        "        None\n",
        "        \n",
        "        # convert the protocol bufffer message to a Python dictionary\n",
        "        example_dict = None\n",
        "        \n",
        "        # append to the records list\n",
        "        None\n",
        "        \n",
        "    ### END CODE HERE\n",
        "    return records"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n5TpA1GyrIXm"
      },
      "source": [
        "# Get 3 records from the dataset\n",
        "sample_records = get_records(dataset, 3)\n",
        "\n",
        "# Print the output\n",
        "pp.pprint(sample_records)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2gluYjccf-IP"
      },
      "source": [
        "You should see three of the examples printed above. Now that `ExampleGen` has finished ingesting the data, the next step is data analysis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "csM6BFhtk5Aa"
      },
      "source": [
        "<a name='2-2'></a>\n",
        "### 2.2 - StatisticsGen\n",
        "The [StatisticsGen](https://www.tensorflow.org/tfx/guide/statsgen) component computes statistics over your dataset for data analysis, as well as for use in downstream components. It uses the [TensorFlow Data Validation](https://www.tensorflow.org/tfx/data_validation/get_started) library.\n",
        "\n",
        "`StatisticsGen` takes as input the dataset ingested using `CsvExampleGen`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0M_Ya16HrIXn"
      },
      "source": [
        "<a name='ex-3'></a>\n",
        "#### Exercise 3: StatisticsGen\n",
        "\n",
        "Fill the code below to generate statistics from the output examples of `CsvExampleGen`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MAscCCYWgA-9"
      },
      "source": [
        "### START CODE HERE\n",
        "# Instantiate StatisticsGen with the ExampleGen ingested dataset\n",
        "statistics_gen = None\n",
        "    \n",
        "\n",
        "# Run the component\n",
        "None\n",
        "### END CODE HERE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HdMJAmzrrIXo"
      },
      "source": [
        "# Plot the statistics generated\n",
        "context.show(statistics_gen.outputs['statistics'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HLKLTO9Nk60p"
      },
      "source": [
        "<a name='2-3'></a>\n",
        "### 2.3 - SchemaGen\n",
        "\n",
        "The [SchemaGen](https://www.tensorflow.org/tfx/guide/schemagen) component also uses TFDV to generate a schema based on your data statistics. As you've learned previously, a schema defines the expected bounds, types, and properties of the features in your dataset.\n",
        "\n",
        "`SchemaGen` will take as input the statistics that we generated with `StatisticsGen`, looking at the training split by default."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wDPHV-xIrIXo"
      },
      "source": [
        "<a name='ex-4'></a>\n",
        "#### Exercise 4: SchemaGen"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ygQvZ6hsiQ_J"
      },
      "source": [
        "### START CODE HERE\n",
        "# Instantiate SchemaGen with the output statistics from the StatisticsGen\n",
        "schema_gen = None\n",
        "    \n",
        "    \n",
        "\n",
        "# Run the component\n",
        "None\n",
        "### END CODE HERE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zi6TxTUKXM6b"
      },
      "source": [
        "If all went well, you can now visualize the generated schema as a table."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ec9vqDXpXeMb"
      },
      "source": [
        "# Visualize the output\n",
        "context.show(schema_gen.outputs['schema'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kZWWdbA-m7zp"
      },
      "source": [
        "Each attribute in your dataset shows up as a row in the schema table, alongside its properties. The schema also captures all the values that a categorical feature takes on, denoted as its domain.\n",
        "\n",
        "This schema will be used to detect anomalies in the next step."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V1qcUuO9k9f8"
      },
      "source": [
        "<a name='2-4'></a>\n",
        "### 2.4 - ExampleValidator\n",
        "\n",
        "The [ExampleValidator](https://www.tensorflow.org/tfx/guide/exampleval) component detects anomalies in your data based on the generated schema from the previous step. Like the previous two components, it also uses TFDV under the hood. \n",
        "\n",
        "`ExampleValidator` will take as input the statistics from `StatisticsGen` and the schema from `SchemaGen`. By default, it compares the statistics from the evaluation split to the schema from the training split."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h6XS_kodrIXp"
      },
      "source": [
        "<a name='2-4'></a>\n",
        "#### Exercise 5: ExampleValidator\n",
        "\n",
        "Fill the code below to detect anomalies in your datasets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XRlRUuGgiXks"
      },
      "source": [
        "### START CODE HERE\n",
        "# Instantiate ExampleValidator with the statistics and schema from the previous steps\n",
        "example_validator = None\n",
        "    \n",
        "    \n",
        "\n",
        "# Run the component\n",
        "None\n",
        "### END CODE HERE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "855mrHgJcoer"
      },
      "source": [
        "As with the previous steps, you can visualize the anomalies as a table."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TDyAAozQcrk3"
      },
      "source": [
        "# Visualize the output\n",
        "context.show(example_validator.outputs['anomalies'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "znMoJj60ybZx"
      },
      "source": [
        "If there are anomalies detected, you should examine how you should handle it. For example, you can relax distribution constraints or modify the domain of some features. You've had some practice with this last week when you used TFDV and you can also do that here. \n",
        "\n",
        "For this particular case, there should be no anomalies detected and we can proceed to the next step."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JPViEz5RlA36"
      },
      "source": [
        "<a name='2-5'></a>\n",
        "### 2.5 - Transform\n",
        "\n",
        "In this section, you will use the [Transform](https://www.tensorflow.org/tfx/guide/transform) component to perform feature engineering.\n",
        "\n",
        "`Transform` will take as input the data from `ExampleGen`, the schema from `SchemaGen`, as well as a module containing the preprocessing function.\n",
        "\n",
        "The component expects an external module for your Transform code so you need to use the magic command `%% writefile` to save the file to disk. We have defined a few constants that group the data's attributes according to the transforms you will perform later. This file will also be saved locally."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PuNSiUKb4YJf"
      },
      "source": [
        "# Set the constants module filename\n",
        "_traffic_constants_module_file = 'traffic_constants.py'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HPjhXuIF4YJh"
      },
      "source": [
        "%%writefile {_traffic_constants_module_file}\n",
        "\n",
        "# Features to be scaled to the z-score\n",
        "DENSE_FLOAT_FEATURE_KEYS = ['temp', 'snow_1h']\n",
        "\n",
        "# Features to bucketize\n",
        "BUCKET_FEATURE_KEYS = ['rain_1h']\n",
        "\n",
        "# Number of buckets used by tf.transform for encoding each feature.\n",
        "FEATURE_BUCKET_COUNT = {'rain_1h': 3}\n",
        "\n",
        "# Feature to scale from 0 to 1\n",
        "RANGE_FEATURE_KEYS = ['clouds_all']\n",
        "\n",
        "# Number of vocabulary terms used for encoding VOCAB_FEATURES by tf.transform\n",
        "VOCAB_SIZE = 1000\n",
        "\n",
        "# Count of out-of-vocab buckets in which unrecognized VOCAB_FEATURES are hashed.\n",
        "OOV_SIZE = 10\n",
        "\n",
        "# Features with string data types that will be converted to indices\n",
        "VOCAB_FEATURE_KEYS = [\n",
        "    'holiday',\n",
        "    'weather_main',\n",
        "    'weather_description'\n",
        "]\n",
        "\n",
        "# Features with int data type that will be kept as is\n",
        "CATEGORICAL_FEATURE_KEYS = [\n",
        "    'hour', 'day', 'day_of_week', 'month'\n",
        "]\n",
        "\n",
        "# Feature to predict\n",
        "VOLUME_KEY = 'traffic_volume'\n",
        "\n",
        "def transformed_name(key):\n",
        "    return key + '_xf'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K0BdF1I9rIXr"
      },
      "source": [
        "<a name='ex-6'></a>\n",
        "#### Exercise 6"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Duj2Ax5z4YJl"
      },
      "source": [
        "Next, you will fill out the transform module. As mentioned, this will also be saved to disk. Specifically, you will complete the `preprocessing_fn` which defines the transformations. See the code comments for instructions and refer to the [tft module documentation](https://www.tensorflow.org/tfx/transform/api_docs/python/tft) to look up which function to use for a given group of keys.\n",
        "\n",
        "For the label (i.e. `VOLUME_KEY`), you will transform it to indicate if it is greater than the mean of the entire dataset. For the transform to work, you will need to convert a [SparseTensor](https://www.tensorflow.org/api_docs/python/tf/sparse/SparseTensor) to a dense one. We've provided a `_fill_in_missing()` helper function for you to use."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4AJ9hBs94YJm"
      },
      "source": [
        "# Set the transform module filename\n",
        "_traffic_transform_module_file = 'traffic_transform.py'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "graded": true,
        "id": "MYmxxx9A4YJn",
        "name": "preprocessing_fn_code"
      },
      "source": [
        "%%writefile {_traffic_transform_module_file}\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_transform as tft\n",
        "\n",
        "import traffic_constants\n",
        "\n",
        "# Unpack the contents of the constants module\n",
        "_DENSE_FLOAT_FEATURE_KEYS = traffic_constants.DENSE_FLOAT_FEATURE_KEYS\n",
        "_RANGE_FEATURE_KEYS = traffic_constants.RANGE_FEATURE_KEYS\n",
        "_VOCAB_FEATURE_KEYS = traffic_constants.VOCAB_FEATURE_KEYS\n",
        "_VOCAB_SIZE = traffic_constants.VOCAB_SIZE\n",
        "_OOV_SIZE = traffic_constants.OOV_SIZE\n",
        "_CATEGORICAL_FEATURE_KEYS = traffic_constants.CATEGORICAL_FEATURE_KEYS\n",
        "_BUCKET_FEATURE_KEYS = traffic_constants.BUCKET_FEATURE_KEYS\n",
        "_FEATURE_BUCKET_COUNT = traffic_constants.FEATURE_BUCKET_COUNT\n",
        "_VOLUME_KEY = traffic_constants.VOLUME_KEY\n",
        "_transformed_name = traffic_constants.transformed_name\n",
        "\n",
        "\n",
        "def preprocessing_fn(inputs):\n",
        "    \"\"\"tf.transform's callback function for preprocessing inputs.\n",
        "    Args:\n",
        "    inputs: map from feature keys to raw not-yet-transformed features.\n",
        "    Returns:\n",
        "    Map from string feature key to transformed feature operations.\n",
        "    \"\"\"\n",
        "    outputs = {}\n",
        "\n",
        "    ### START CODE HERE\n",
        "    \n",
        "    # Scale these features to the z-score.\n",
        "    for key in _DENSE_FLOAT_FEATURE_KEYS:\n",
        "        # Scale these features to the z-score.\n",
        "        outputs[_transformed_name(key)] = None\n",
        "            \n",
        "\n",
        "    # Scale these feature/s from 0 to 1\n",
        "    for key in _RANGE_FEATURE_KEYS:\n",
        "        outputs[_transformed_name(key)] = None\n",
        "            \n",
        "\n",
        "    # Transform the strings into indices \n",
        "    # hint: use the VOCAB_SIZE and OOV_SIZE to define the top_k and num_oov parameters\n",
        "    for key in _VOCAB_FEATURE_KEYS:\n",
        "        outputs[_transformed_name(key)] = None\n",
        "            \n",
        "            \n",
        "            \n",
        "\n",
        "    # Bucketize the feature\n",
        "    for key in _BUCKET_FEATURE_KEYS:\n",
        "        outputs[_transformed_name(key)] = None\n",
        "            \n",
        "            \n",
        "\n",
        "    # Keep as is. No tft function needed.\n",
        "    for key in _CATEGORICAL_FEATURE_KEYS:\n",
        "        outputs[_transformed_name(key)] = None\n",
        "\n",
        "        \n",
        "    # Use `tf.cast` to cast the label key to float32 and fill in the missing values.\n",
        "    traffic_volume = None\n",
        "  \n",
        "    \n",
        "    # Create a feature that shows if the traffic volume is greater than the mean and cast to an int\n",
        "    outputs[_transformed_name(_VOLUME_KEY)] = tf.cast(  \n",
        "        \n",
        "        # Use `tf.greater` to check if the traffic volume in a row is greater than the mean of the entire traffic volumn column\n",
        "        tf.greater(None, None(tf.cast(inputs[_VOLUME_KEY], tf.float32))),\n",
        "        \n",
        "        tf.int64)                                        \n",
        "\n",
        "    ### END CODE HERE\n",
        "    return outputs\n",
        "\n",
        "\n",
        "def _fill_in_missing(x):\n",
        "    \"\"\"Replace missing values in a SparseTensor and convert to a dense tensor.\n",
        "    Fills in missing values of `x` with '' or 0, and converts to a dense tensor.\n",
        "    Args:\n",
        "        x: A `SparseTensor` of rank 2.  Its dense shape should have size at most 1\n",
        "          in the second dimension.\n",
        "    Returns:\n",
        "        A rank 1 tensor where missing values of `x` have been filled in.\n",
        "    \"\"\"\n",
        "    default_value = '' if x.dtype == tf.string else 0\n",
        "    \n",
        "    return tf.squeeze(\n",
        "      tf.sparse.to_dense(\n",
        "          tf.SparseTensor(x.indices, x.values, [x.dense_shape[0], 1]),\n",
        "          default_value),\n",
        "      axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OM525Y7OrIXs"
      },
      "source": [
        "<a name='ex-7'></a>\n",
        "#### Exercise 7\n",
        "\n",
        "With the transform module defined, complete the code below to perform feature engineering on the raw data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jHfhth_GiZI9"
      },
      "source": [
        "# ignore tf warning messages\n",
        "tf.get_logger().setLevel('ERROR')\n",
        "\n",
        "\n",
        "### START CODE HERE\n",
        "# Instantiate the Transform component\n",
        "transform = None\n",
        "    \n",
        "    \n",
        "    \n",
        "\n",
        "# Run the component.\n",
        "# The `enable_cache` flag is disabled in case you need to update your transform module file.\n",
        "context.run(transform, enable_cache=False)\n",
        "### END CODE HERE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fwAwb4rARRQ2"
      },
      "source": [
        "You should see the output cell by `InteractiveContext` above and see the three artifacts in `.component.outputs`:\n",
        "\n",
        "* `transform_graph` is the graph that performs the preprocessing operations. This will be included during training and serving to ensure consistent transformations of incoming data.\n",
        "* `transformed_examples` points to the preprocessed training and evaluation data.\n",
        "* `updated_analyzer_cache` are stored calculations from previous runs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vyFkBd9AR1sy"
      },
      "source": [
        "The `transform_graph` artifact URI should point to a directory containing:\n",
        "\n",
        "* The `metadata` subdirectory containing the schema of the original data.\n",
        "* The `transformed_metadata` subdirectory containing the schema of the preprocessed data. \n",
        "* The `transform_fn` subdirectory containing the actual preprocessing graph.\n",
        "\n",
        "Again, for grading purposes, we inserted an `except` and `else` below to handle checking the output outside the notebook environment."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5tRw4DneR3i7"
      },
      "source": [
        "try:\n",
        "    # Get the uri of the transform graph\n",
        "    transform_graph_uri = transform.outputs['transform_graph'].get()[0].uri\n",
        "\n",
        "except IndexError:\n",
        "    print(\"context.run() was no-op\")\n",
        "    transform_path = './metro_traffic_pipeline/Transform/transformed_examples'\n",
        "    dir_id = os.listdir(transform_path)[0]\n",
        "    transform_graph_uri = f'{transform_path}/{dir_id}'\n",
        "    \n",
        "else:\n",
        "    # List the subdirectories under the uri\n",
        "    os.listdir(transform_graph_uri)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Al9BOkFkrIXt"
      },
      "source": [
        "Lastly, you can also take a look at a few of the transformed examples."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IZPQykRHrIXt"
      },
      "source": [
        "try:\n",
        "    # Get the URI of the output artifact representing the transformed examples\n",
        "    train_uri = os.path.join(transform.outputs['transformed_examples'].get()[0].uri, 'train')\n",
        "    \n",
        "except IndexError:\n",
        "    print(\"context.run() was no-op\")\n",
        "    train_uri = os.path.join(transform_graph_uri, 'train')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pwbW2zPKR_S4"
      },
      "source": [
        "# Get the list of files in this directory (all compressed TFRecord files)\n",
        "tfrecord_filenames = [os.path.join(train_uri, name)\n",
        "                      for name in os.listdir(train_uri)]\n",
        "\n",
        "# Create a `TFRecordDataset` to read these files\n",
        "transformed_dataset = tf.data.TFRecordDataset(tfrecord_filenames, compression_type=\"GZIP\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mSDZ2rJC7NQW"
      },
      "source": [
        "# Get 3 records from the dataset\n",
        "sample_records_xf = get_records(transformed_dataset, 3)\n",
        "\n",
        "# Print the output\n",
        "pp.pprint(sample_records_xf)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E_9iknyerIXt"
      },
      "source": [
        "**Congratulations on completing this week's assignment!** You've just demonstrated how to build a data pipeline and do feature engineering. You will build upon these concepts in the next weeks where you will deal with more complex datasets and also access the metadata store. Keep up the good work!"
      ]
    }
  ]
}